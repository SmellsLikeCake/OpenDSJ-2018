{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract excel and pdf files from https://www.southtechhosting.com/SanJoseCity/CampaignDocsWebRetrieval/Search/SearchByElection.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import os,sys\n",
    "from os.path import isfile,join\n",
    "from os import listdir\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_form_text(top_form_xpath,bottom_form_xpath):\n",
    "    '''\n",
    "    scrape information from forms and save to list to use for naming folders or files\n",
    "    '''\n",
    "    data = []\n",
    "    data_text = []\n",
    "\n",
    "    for row in top_form_xpath:\n",
    "        data.append(row)\n",
    "\n",
    "    if bottom_form_xpath:\n",
    "        data.append(bottom_form_xpath[0])\n",
    "    \n",
    "    for text_path in data:\n",
    "        data_text.append(text_path.text)\n",
    "        \n",
    "    return data_text\n",
    "\n",
    "\n",
    "def downloads_done():\n",
    "    '''\n",
    "    recursive function that checks to see if pdf is fully downloaded before proceeding to next pdf file\n",
    "    '''\n",
    "    for i in os.listdir(new_path):\n",
    "        if \".crdownload\" in i:\n",
    "            sleep(0.5)\n",
    "            downloads_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory --> change to data directory\n",
    "path = os.getcwd()\n",
    "create_data_folder = os.path.join(path,\"data\")\n",
    "if not os.path.exists(create_data_folder):\n",
    "    os.makedirs(create_data_folder)\n",
    "folders = os.listdir(os.getcwd())\n",
    "for folder in folders:\n",
    "    if os.path.join(path,folder) == create_data_folder:\n",
    "        abs_file_path = os.path.join(path,folder)\n",
    "        os.chdir(abs_file_path)\n",
    "new_path = abs_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate selenium chrome webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "# remove window/browser features (can choose to remove in order to test program)\n",
    "options.add_argument('headless')\n",
    "options.add_argument(\"--ignore-certificate-errors\")\n",
    "options.add_argument(\"--test_type\")\n",
    "# set download path to \"data\" folder created in previous step (new_path), also disable default pdf viewer (easier downloading)\n",
    "prefs = {\"download.default_directory\":new_path,\"plugins.plugins_list\":[{\"enabled\":False,\"name\":\"Chrome PDF Viewer\"}]}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# initiate driver and call southtech url\n",
    "driver.get(\"https://www.southtechhosting.com/SanJoseCity/CampaignDocsWebRetrieval/Search/SearchByElection.aspx\")\n",
    "# wait for page to respond before applying selenium actions via xpath\n",
    "try:\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID,'ctl00_DefaultContent_ASPxRoundPanel1_btnFindFilers_CD')))\n",
    "except TimeoutException:\n",
    "    print(\"Loading took too long.\")\n",
    "    \n",
    "# find \"submit\" button and click \n",
    "driver.find_element_by_xpath('//*[@id=\"ctl00_DefaultContent_ASPxRoundPanel1_btnFindFilers_CD\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this works! Clean up code and add file downloader and file mover/creator\n",
    "page_track = 0\n",
    "while True:\n",
    "    forms = driver.find_elements_by_xpath('//a[@class=\"dxbButton_Glass dxgvCommandColumnItem_Glass dxgv__cci dxbButtonSys\"]')\n",
    "    sleep(1)\n",
    "    # grab and store form text on home page\n",
    "    outer_top_table_rows = driver.find_elements_by_xpath('//tr[@class=\"dxgvDataRow_Glass\"]')\n",
    "    outer_bottom_table_row = driver.find_elements_by_xpath('//tr[@class=\"dxgvDataRow_Glass dxgvLVR\"]')\n",
    "    outer_form_text = grab_form_text(outer_top_table_rows,outer_bottom_table_row)\n",
    "    \n",
    "    # iterate through pdf and excel files then download\n",
    "    for ind,form in enumerate(forms):\n",
    "        sleep(2)\n",
    "        forms = driver.find_elements_by_xpath('//a[@class=\"dxbButton_Glass dxgvCommandColumnItem_Glass dxgv__cci dxbButtonSys\"]')\n",
    "        forms[ind].click()\n",
    "        sleep(2)\n",
    "        forms = driver.find_elements_by_xpath('//table[@class=\"dxgvControl_Glass dxgv\"]')\n",
    "        # download excel files\n",
    "        excel = driver.find_elements_by_xpath('//td[@class=\"dxgvCommandColumn_Glass dxgv\"]//img[@title=\"Export Transaction Details To Excel\"]')\n",
    "        sleep(2)\n",
    "        for file in excel:\n",
    "            file.click()\n",
    "        sleep(2)\n",
    "        # download pdf files\n",
    "        pdfs = driver.find_elements_by_xpath('//td[@class=\"dxgvCommandColumn_Glass dxgv\"]//img[@title=\"View Form\"]')\n",
    "        for ind2 in range(0,len(pdfs)):\n",
    "            driver.find_elements_by_xpath('//td[@class=\"dxgvCommandColumn_Glass dxgv\"]//img[@title=\"View Form\"]')[ind2].click()\n",
    "            sleep(3)\n",
    "            # switch to pdf window\n",
    "            driver.switch_to.frame(driver.find_element_by_tag_name('iframe'))\n",
    "            \n",
    "            # wait for new page to load before finding \"Click here\" element\n",
    "            delay = 10\n",
    "            try:\n",
    "                WebDriverWait(driver,delay).until(EC.presence_of_element_located((By.LINK_TEXT,\"Click here\")))\n",
    "            except TimeoutException:\n",
    "                print(\"Loading took too much time...\")\n",
    "            # press \"click here\" then download pdf\n",
    "            a = driver.find_element_by_link_text(\"Click here\")\n",
    "            ActionChains(driver).key_down(Keys.CONTROL).click(a).key_up(Keys.CONTROL).perform()\n",
    "            sleep(3)\n",
    "            # switch back to pdf window \n",
    "            driver.switch_to.default_content()\n",
    "            # click heading bar of window to allow \"escape\" action to be performed\n",
    "            driver.find_elements_by_xpath(\"//div[@id='ctl00_GenericPopupSizeable_InnerPopupControl_PWH-1']\")[0].click()\n",
    "            webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "            # recursive function to check if pdf is finished downloading before proceeding to next file\n",
    "            downloads_done()\n",
    "            sleep(3)\n",
    "\n",
    "        # create new folder for storing pdfs/excel files for certain group\n",
    "        raw_str = outer_form_text[ind]\n",
    "        clean_str = re.sub('[^A-Za-z0-9]+','',raw_str)[:175].lower()\n",
    "        add_new_folder = os.path.join(new_path,clean_str)\n",
    "\n",
    "        if not os.path.exists(add_new_folder):\n",
    "            os.makedirs(add_new_folder)\n",
    "\n",
    "        # move files to new folder\n",
    "        root_src_dir = new_path\n",
    "        root_dst_dir = add_new_folder\n",
    "        files = [f for f in listdir(new_path) if isfile(join(new_path,f))]\n",
    "\n",
    "        for file in files:\n",
    "            shutil.move(os.path.join(new_path,file), root_dst_dir)\n",
    "\n",
    "        # click back button\n",
    "        if driver.find_elements_by_xpath('//*[@id=\"ctl00_DefaultContent_buttonBack\"]'):\n",
    "            driver.find_elements_by_xpath('//*[@id=\"ctl00_DefaultContent_buttonBack\"]')[0].click()\n",
    "        else:\n",
    "            driver.find_elements_by_xpath('//*[@id=\"ctl00_DefaultContent_buttonBack_CD\"]')[0].click()\n",
    "        sleep(1)\n",
    "\n",
    "        '''\n",
    "        Modify this next section of code so that it can be applied to different site (i.e. another city using south tech hosting)\n",
    "        Currently set to length of pages on site containing files (9 different pages) with the last page having only 3 files\n",
    "        Want to make more robust in case file or page numbers change with site updates...\n",
    "        '''\n",
    "        if page_track == 8 and ind == 2:\n",
    "            ind = 9 \n",
    "        # hitting back button makes pages reset, need to click pages to get back to relevant files (hacky)\n",
    "        if page_track > 0 and ind < 9 :\n",
    "            next_page = driver.find_elements_by_xpath('//a[@class=\"dxp-button dxp-bi\"]')\n",
    "            sleep(2)\n",
    "            next_page[0].click()\n",
    "            sleep(2)\n",
    "            num = 1\n",
    "            if page_track > 1:  \n",
    "                while True:\n",
    "                    sleep(1)\n",
    "                    next_page = driver.find_elements_by_xpath('//a[@class=\"dxp-button dxp-bi\"]')\n",
    "                    sleep(2)\n",
    "                    if num != page_track:\n",
    "                        next_page[1].click()\n",
    "                        num+=1\n",
    "                        sleep(1)\n",
    "                    else:\n",
    "                        break\n",
    "    # iterate through pages on site (hacky)      \n",
    "    page_track += 1\n",
    "    next_page = driver.find_elements_by_xpath('//a[@class=\"dxp-button dxp-bi\"]')\n",
    "    if page_track < 9:\n",
    "        next_page[0].click()\n",
    "        if page_track > 1:\n",
    "        # create function to make page clicks \n",
    "            num = 1\n",
    "            while True:\n",
    "                sleep(1)\n",
    "                next_page = driver.find_elements_by_xpath('//a[@class=\"dxp-button dxp-bi\"]')\n",
    "                sleep(2)\n",
    "                if num != page_track:\n",
    "                    next_page[1].click()\n",
    "                    num+=1\n",
    "                    sleep(1)\n",
    "                else:\n",
    "                    break\n",
    "            sleep(1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "'''\n",
    "Some possible additions to the scraper include:\n",
    "1) modifying \"page_track\" to find max number of pages on site and to stop when max has been reached\n",
    "2) last page has less files to download than other pages (causes problems if scraper applied to other sites)\n",
    "3) program is really slow, figure out ways to make faster (replacing nested for loops, etc.)\n",
    "4) figure out how to update files by checking files alrdy downloaded and skipping to new files and creating new folder\n",
    "(problem is currently need to restart program, delete files, and start over again when faced with an error...)\n",
    "5) naming pdf and excel files\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_file(files):\n",
    "#     for ind,file in enumerate(files):\n",
    "#         if files[ind] == \"PdfHandler.pdf\":\n",
    "#             files.insert(0,files.pop(ind))\n",
    "#     return files\n",
    "\n",
    "# # # sort list of strings containing numerical values\n",
    "# # def atof(text):\n",
    "# #     try:\n",
    "# #         retval = float(text)\n",
    "# #     except ValueError:\n",
    "# #         retval = text\n",
    "# #     return retval\n",
    "\n",
    "# # def natural_keys(text):\n",
    "# #     return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of pdf files (rough outline)\n",
    "# grab form text\n",
    "# inner_top_table_rows = driver.find_elements_by_xpath('//tr[@class=\"dxgvDataRow_Glass\"]')\n",
    "# inner_bottom_table_row = driver.find_elements_by_xpath('//tr[@class=\"dxgvDataRow_Glass dxgvLVR\"]')\n",
    "# inner_form_text = grab_form_text(inner_top_table_rows,inner_bottom_table_row)\n",
    "# onlyfiles = [f for f in listdir(new_path) if isfile(join(new_path,f))]\n",
    "# save copy for later\n",
    "# file_copy = onlyfiles.copy()\n",
    "# change to new folder directly\n",
    "#         new_folder = os.listdir(os.getcwd())\n",
    "#         for folder in new_folder:\n",
    "#             if os.path.join(new_path,folder) == add_new_folder:\n",
    "#                 new_dir_path = os.path.join(new_path,folder)\n",
    "#                 os.chdir(new_dir_path)\n",
    "#                 onlyfiles = [f for f in listdir(new_dir_path) if isfile(join(new_dir_path,f))]\n",
    "        \n",
    "#                 # sort and place first pdf file to front of list\n",
    "#                 if len(onlyfiles) > 1:\n",
    "#                     onlyfiles.sort(key=natural_keys)\n",
    "#                     onlyfiles = insert_file(onlyfiles)\n",
    "        \n",
    "# #                 # rename file names\n",
    "# #                 for ind3,file in enumerate(onlyfiles):\n",
    "# #                     if ind3 == 12:\n",
    "# #                         break\n",
    "# #                     onlyfiles[ind3] = inner_form_text[ind3] + \".pdf\"\n",
    "# #                     onlyfiles[ind3] = onlyfiles[ind3].replace(\"/\",\"\").replace(\"-\",\" \").replace(\" \",\"\")\n",
    "        \n",
    "#                 # rename files in new folder with modified file names\n",
    "#                 file_copy.sort(key=natural_keys)\n",
    "\n",
    "#                 file_copy = insert_file(file_copy)\n",
    "    \n",
    "#                 for ind4,file in enumerate(onlyfiles):\n",
    "#                     os.renames(file_copy[ind4],onlyfiles[ind4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.find_elements_by_xpath('//*[@id=\"ctl00_DefaultContent_buttonBack\"]')[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how to get all excel & pdf files from all links on home page (1-9)\n",
    "# click \"Back\" after downloading all pdf and excel files\n",
    "# Click next number page on bottom of main page to get new campaign files \n",
    "# Repeat process until all files have been downloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
